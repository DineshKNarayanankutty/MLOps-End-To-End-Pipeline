# End-to-End MLOps Pipeline (Iris Classification)

## ğŸ“– Project Overview

This project demonstrates how a **machine learning model is built, packaged, deployed, and operated as a production service**, rather than remaining as a notebook or standalone script.

The **Iris classification problem** is used as a simple, well-known ML task so that the focus remains on **system architecture, deployment, and operations** instead of model complexity.

At a high level, this project covers:

* Training a reproducible ML model
* Tracking experiments and artifacts with MLflow
* Serving predictions through a REST API
* Packaging the application using Docker
* Storing images in Azure Container Registry (ACR)
* Deploying the service to Azure Kubernetes Service (AKS)
* Observing runtime behavior with Prometheus and Grafana

---

## ğŸ¯ Core Intention

The core intention of this project is to show that:

> **A trained ML model only becomes valuable when it can run reliably as a service.**

Rather than focusing on algorithm tuning, this project emphasizes the operational side of ML systems:

| Area       | What This Project Demonstrates      |
| ---------- | ----------------------------------- |
| Training   | Script-driven, repeatable training  |
| Tracking   | Versioned experiments and artifacts |
| Serving    | API-based access to the model       |
| Packaging  | Immutable Docker images             |
| Deployment | Kubernetes-based runtime (AKS)      |
| Operations | Health checks and metrics           |

The model is treated as **software that must be built, deployed, and operated**, not as a one-time experiment.

---

## ğŸŒ¸ Why the Iris Dataset?

The Iris dataset is intentionally chosen because it is:

* Small and fast to train
* Easy to understand without deep ML knowledge
* Free from complex data engineering requirements

This allows anyone reading the project to focus on **how the system works end to end**. The same architecture can be reused for larger, real-world datasets without changing the overall design.

---

## ğŸ§  End-to-End Flow

```
Load Data
   â†“
Train Model
   â†“
Log Metrics & Model Artifact (MLflow)
   â†“
Package API with Docker
   â†“
Push Image to Azure Container Registry (ACR)
   â†“
Deploy to Azure Kubernetes Service (AKS)
   â†“
Serve Predictions via REST API
   â†“
Expose Metrics â†’ Prometheus â†’ Grafana
```

External users interact only with the **API**, not with the training or model management components.

---

## ğŸ—ï¸ High-Level Architecture

![End-to-End MLOps System Architecture](docs/End-to-End MLOps System Architecture-WM1.png)

## ğŸ–‡ï¸ Logical Flow

```
Training Script
   â”‚
   â–¼
MLflow (Experiments & Artifacts)
   â”‚
   â–¼
FastAPI Application
   â”‚
   â–¼
Docker Image
   â”‚
   â–¼
Azure Container Registry (ACR)
   â”‚
   â–¼
Azure Kubernetes Service (AKS)
   â”‚
   â–¼
Prometheus â†’ Grafana
```

---

## ğŸ“ Project Structure Explained

```
MLOps-End-To-End-Pipeline/
â”‚
â”œâ”€â”€ api/                     # FastAPI application (model serving)
â”‚   â””â”€â”€ main.py              # API endpoints, health, metrics
â”‚
â”œâ”€â”€ src/                     # Core ML logic
â”‚   â”œâ”€â”€ data_loader.py       # Dataset loading
â”‚   â”œâ”€â”€ preprocessing.py     # Feature preprocessing
â”‚   â”œâ”€â”€ training.py          # Model training logic
â”‚   â”œâ”€â”€ model_registry.py    # MLflow model registration
â”‚   â””â”€â”€ drift_detector.py    # Drift detection logic
â”‚
â”œâ”€â”€ scripts/                 # Pipeline automation
â”‚   â”œâ”€â”€ train_pipeline.py    # End-to-end training pipeline
â”‚   â””â”€â”€ model_promotion.py   # Model promotion logic
â”‚
â”œâ”€â”€ tests/                   # Unit & API tests
â”‚   â”œâ”€â”€ test_api.py
â”‚   â”œâ”€â”€ test_data_loader.py
â”‚   â”œâ”€â”€ test_preprocessing.py
â”‚   â””â”€â”€ test_training.py
â”‚
â”œâ”€â”€ k8s/                     # Kubernetes manifests
â”‚   â”œâ”€â”€ app/                 # API deployment & service
â”‚   â””â”€â”€ observability/       # Prometheus, Grafana, MLflow
â”‚
â”œâ”€â”€ .github/workflows/       # GitHub Actions CI/CD pipelines
â”‚   â”œâ”€â”€ tests.yml            # Run tests
â”‚   â”œâ”€â”€ build.yml            # Build & push Docker image to ACR
â”‚   â”œâ”€â”€ deploy.yml           # Deploy to AKS
â”‚   â””â”€â”€ observability.yml    # Deploy monitoring stack
â”‚
â”œâ”€â”€ Dockerfile               # Container definition
â”œâ”€â”€ docker-compose.yml       # Local multi-service setup
â”œâ”€â”€ requirements.txt         # Runtime dependencies
â”œâ”€â”€ requirements-dev.txt     # Development & testing dependencies
â””â”€â”€ README.md
```

---

## ğŸ”¬ Machine Learning Details

| Aspect       | Description                               |
| ------------ | ----------------------------------------- |
| Problem Type | Multiclass classification                 |
| Dataset      | Iris dataset                              |
| Library      | scikit-learn                              |
| Metrics      | Accuracy and basic classification metrics |

The ML implementation is intentionally **simple and modular**, allowing the system architecture to remain the primary focus.

---

## ğŸ” Training & Experiment Tracking

Training is executed via a Python script:

```bash
python scripts/train_pipeline.py
```

During training:

* Parameters and metrics are logged to **MLflow**
* The trained model is stored as a **versioned artifact**

This enables reproducibility and comparison across runs.

---

## ğŸŒ Model Serving (FastAPI)

The FastAPI service exposes:

* `POST /predict` â€“ return model predictions
* `GET /health` â€“ service health status
* `GET /metrics` â€“ Prometheus-compatible metrics

The model is loaded at application startup and used only for inference.

---

## ğŸ³ Containerization

The FastAPI application is packaged using Docker to ensure consistent behavior across environments.

```bash
docker build -t mlops-api .
```

The image is pushed to **Azure Container Registry (ACR)**:

```bash
docker tag mlops-api <acr-name>.azurecr.io/mlops-api:latest
docker push <acr-name>.azurecr.io/mlops-api:latest
```

---

## â˜¸ï¸ Kubernetes Deployment (AKS)

The containerized application is deployed to **Azure Kubernetes Service (AKS)**.

Deployment flow:

```
GitHub Actions â†’ Azure Container Registry (ACR) â†’ AKS
```

Kubernetes is responsible for:

* Running application pods
* Restarting failed containers
* Exposing the API through a Kubernetes Service

---

## ğŸ“Š Monitoring & Observability

* **Prometheus** scrapes metrics from the `/metrics` endpoint
* **Grafana** visualizes request counts, latency, and service health

This provides visibility into how the system behaves after deployment.

---

## ğŸ”„ CI/CD with GitHub Actions

GitHub Actions automates the delivery pipeline:

1. Run unit and API tests
2. Build the Docker image
3. Push the image to **Azure Container Registry (ACR)**
4. Deploy the updated image to **AKS**

This ensures every deployment uses a tested, versioned container image.

---

## â–¶ï¸ Running Locally

```bash
python -m venv venv
source venv/bin/activate  # Linux / Mac
venv\Scripts\activate     # Windows

pip install -r requirements.txt
uvicorn api.main:app --reload
```

---

## ğŸ”® Future Improvements

* Automated retraining pipelines
* Advanced drift detection
* Canary or blue-green deployments
* Feature store integration
* Model explainability dashboards

---

## ğŸ‘¤ Author

This project was built to demonstrate **practical MLOps system design**, focusing on clarity, correctness, and real-world deployment patterns.

---

â­ **Key takeaway:** this repository shows how a simple ML model can be transformed into a reliable, observable production service.
