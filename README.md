# End-to-End MLOps Pipeline (Iris Classification)

## ğŸ“– Project Overview

This project demonstrates how a **machine learning model is built, packaged, and served as a running system**, rather than remaining as a notebook or standalone script.

Using the **Iris classification problem** as a simple and wellâ€‘understood example, the project focuses on the *engineering side* of ML systems:

* Training a reproducible ML model
* Tracking experiments and artifacts using MLflow
* Serving predictions through a REST API
* Packaging the application with Docker
* Deploying the service to Kubernetes (AKS)
* Exposing basic system and application metrics

The ML problem itself is intentionally simple so that the attention stays on **system design, deployment, and operability**.

---

## ğŸ¯ Core Intention

The core intention of this project is to show that:

> **A trained ML model only becomes useful when it can run reliably as a service.**

Instead of optimizing algorithms, this project emphasizes:

| Focus Area | What This Project Demonstrates          |
| ---------- | --------------------------------------- |
| Training   | Repeatable, scriptâ€‘driven training      |
| Tracking   | Versioned experiments and models        |
| Serving    | APIâ€‘based model access                  |
| Deployment | Containerized, Kubernetesâ€‘based runtime |
| Operations | Health checks and basic observability   |

The project treats the model as **software that must be built, deployed, and operated**, not as a oneâ€‘time experiment.

---

## ğŸŒ¸ Why the Iris Dataset?

The Iris dataset is used deliberately because it is:

* Small and fast to train
* Easy to understand without ML expertise
* Free from heavy dataâ€‘engineering requirements

This keeps the focus on **how the system works**, not on dataset complexity. The same architecture can be reused for larger datasets without changing the overall design.

---

## ğŸ§  Endâ€‘toâ€‘End Flow

```
Load Data
   â†“
Train Model
   â†“
Log Metrics & Artifacts (MLflow)
   â†“
Serve Model via FastAPI
   â†“
Package with Docker
   â†“
Deploy to Kubernetes (AKS)
   â†“
Expose Metrics & Health Endpoints
```

At runtime, external users or services interact only with the **API**, not directly with the model or training code.

---

## ğŸ—ï¸ Highâ€‘Level Architecture

```
Training Script
   â”‚
   â–¼
MLflow (Experiments & Artifacts)
   â”‚
   â–¼
FastAPI Application
   â”‚
   â–¼
Docker Image
   â”‚
   â–¼
Kubernetes (AKS)
   â”‚
   â–¼
Prometheus â†’ Grafana
```

---

## ğŸ“ Project Structure Explained

```
MLOps-End-To-End-Pipeline/
â”‚
â”œâ”€â”€ api/                     # FastAPI application (model serving)
â”‚   â””â”€â”€ main.py              # API endpoints, health, metrics
â”‚
â”œâ”€â”€ src/                     # Core ML logic
â”‚   â”œâ”€â”€ data_loader.py       # Dataset loading
â”‚   â”œâ”€â”€ preprocessing.py     # Feature preprocessing
â”‚   â”œâ”€â”€ training.py          # Model training logic
â”‚   â”œâ”€â”€ model_registry.py    # MLflow model registration
â”‚   â””â”€â”€ drift_detector.py    # Drift detection logic
â”‚
â”œâ”€â”€ scripts/                 # Pipeline automation
â”‚   â”œâ”€â”€ train_pipeline.py    # End-to-end training pipeline
â”‚   â””â”€â”€ model_promotion.py   # Model promotion logic
â”‚
â”œâ”€â”€ tests/                   # Unit & API tests
â”‚   â”œâ”€â”€ test_api.py
â”‚   â”œâ”€â”€ test_data_loader.py
â”‚   â”œâ”€â”€ test_preprocessing.py
â”‚   â””â”€â”€ test_training.py
â”‚
â”œâ”€â”€ k8s/                     # Kubernetes manifests
â”‚   â”œâ”€â”€ app/                 # API deployment & service
â”‚   â””â”€â”€ observability/       # Prometheus, Grafana, MLflow
â”‚
â”œâ”€â”€ .github/workflows/       # GitHub Actions CI/CD pipelines
â”‚   â”œâ”€â”€ tests.yml            # Run tests
â”‚   â”œâ”€â”€ build.yml            # Build & push Docker image
â”‚   â”œâ”€â”€ deploy.yml           # Deploy to AKS
â”‚   â””â”€â”€ observability.yml    # Deploy monitoring stack
â”‚
â”œâ”€â”€ Dockerfile               # Container definition
â”œâ”€â”€ docker-compose.yml       # Local multi-service setup
â”œâ”€â”€ requirements.txt         # Runtime dependencies
â”œâ”€â”€ requirements-dev.txt     # Development & testing dependencies
â””â”€â”€ README.md
```

---

## ğŸ”¬ Machine Learning Details

| Aspect       | Description                               |
| ------------ | ----------------------------------------- |
| Problem Type | Multiclass classification                 |
| Dataset      | Iris dataset                              |
| Library      | scikitâ€‘learn                              |
| Metrics      | Accuracy and basic classification metrics |

The ML code is intentionally **simple and modular**, making it easy to replace the model without changing the system architecture.

---

## ğŸ” Training & Experiment Tracking

Training is executed using a Python script:

```bash
python scripts/train_pipeline.py
```

During training:

* Parameters and metrics are logged to **MLflow**
* The trained model artifact is stored for later use

This enables reproducibility and comparison between runs.

---

## ğŸŒ Model Serving (FastAPI)

The FastAPI service exposes:

* `POST /predict` â€“ returns model predictions
* `GET /health` â€“ basic service health information
* `GET /metrics` â€“ Prometheusâ€‘compatible metrics

The API loads the trained model at startup and serves predictions over HTTP.

---

## ğŸ³ Containerization

The application is packaged using Docker to ensure consistent runtime behavior:

```bash
docker build -t mlops-api .
docker run -p 8000:8000 mlops-api
```

---

## â˜¸ï¸ Kubernetes Deployment (AKS)

The containerized API is deployed to **Azure Kubernetes Service (AKS)** using Kubernetes manifests.

Kubernetes handles:

* Running the API pods
* Restarting failed containers
* Exposing the service via a Kubernetes Service

---

## ğŸ“Š Monitoring & Observability

* **Prometheus** scrapes metrics from the `/metrics` endpoint
* **Grafana** visualizes request counts, latency, and service health

This provides basic visibility into how the system behaves after deployment.

---

## ğŸ”„ CI/CD with GitHub Actions

GitHub Actions automate:

* Running tests on every change
* Building Docker images
* Deploying updated versions to AKS

This ensures that changes are validated and deployed in a repeatable way.

---

## â–¶ï¸ Running Locally

```bash
python -m venv venv
source venv/bin/activate  # Linux / Mac
venv\Scripts\activate     # Windows

pip install -r requirements.txt
uvicorn api.main:app --reload
```

---

## ğŸ”® Future Improvements

* Automated retraining pipelines
* More advanced drift detection
* Canary or blueâ€‘green deployments
* Feature store integration
* Model explainability dashboards

---

## ğŸ‘¤ Author

This project was built to demonstrate **practical MLOps system design**, focusing on clarity, correctness, and realâ€‘world deployment patterns.

---

â­ **Key takeaway:** this repository shows how a simple ML model can be transformed into a reliable, observable production service.
